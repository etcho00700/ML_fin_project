{"cells":[{"cell_type":"markdown","source":["**Creating 2000 fake X-ray images (1000 for normal x-ray and 1000 for covid x-ray) using Deep Convolutional Generative Adversarial Network**"],"metadata":{"id":"8MKjr8uNzMQQ"}},{"cell_type":"markdown","source":["To see the results of the sequential model performance with DCGAN generated augmented images, go to:\n","https://github.com/etcho00700/ML_fin_project.git\n","\n","The repo contains the datasets we used to train and the end results. "],"metadata":{"id":"889ZSq_w01i4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dgSx2xxqMYn_"},"outputs":[],"source":["from skimage import img_as_float\n","from skimage import exposure\n","from skimage import io # To preprocess the images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CXPfOnVvUlVO"},"outputs":[],"source":["\n","import matplotlib.pyplot as plt \n","import matplotlib.animation as animation\n","from IPython.display import HTML\n","\n","\n","import sys, os, glob, time, imageio \n","import numpy as np, pandas as pd  \n","from PIL import Image \n","\n","\n","import torch \n","import torchvision.utils as vutils \n","import torchvision.transforms as transforms \n","import tensorflow as tf \n","from tensorflow.keras import models, layers, optimizers \n","from tensorflow.keras.models import Sequential \n","from tensorflow.keras.preprocessing.image import array_to_img, img_to_array, load_img "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16979,"status":"ok","timestamp":1639511418267,"user":{"displayName":"Chanha Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh2AHZ0FrANs7IKDgS2gQWeYvsdS5NCVRIys5z9dw=s64","userId":"13291631259498353023"},"user_tz":300},"id":"NH4pLvsC0y_t","outputId":"1b673084-02ff-42be-984e-ee52dc3b1395"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EVeQRWSb40Kg"},"outputs":[],"source":["!unzip drive/MyDrive/Covid-dataset.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40,"status":"ok","timestamp":1639511449827,"user":{"displayName":"Chanha Kim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh2AHZ0FrANs7IKDgS2gQWeYvsdS5NCVRIys5z9dw=s64","userId":"13291631259498353023"},"user_tz":300},"id":"uZVdkOFO5zlh","outputId":"0c308db9-cc3c-4901-d59d-d2933e841322"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/COVID-19_Radiography_Dataset\n"]}],"source":["cd COVID-19_Radiography_Dataset/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a1RNdm9cJ-ry"},"outputs":[],"source":["# Create new folders for image training\n","!mkdir ./COVID1500/\n","!mkdir ./NORMAL1500/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rEyZzVUz1rVa"},"outputs":[],"source":["# Set dataset path\n","DATASET_PATH = '/content/COVID-19_Radiography_Dataset'\n","\n","# There are two classes of images that we will deal with\n","cls = ['COVID', 'Normal']\n","\n","covid_path = os.path.join(DATASET_PATH, cls[0], '*')\n","normal_path = os.path.join(DATASET_PATH, cls[1], '*')\n","\n","# Lists for access paths\n","listCovidPaths = []\n","listNormalPaths = []\n","\n","# Get covid images files paths\n","for root, directories, files in os.walk(covid_path[:-2]):\n","    for name in files:\n","        listCovidPaths.append(os.path.join(root, name))\n","        \n","# Get normal images files paths\n","for root, directories, files in os.walk(normal_path[:-2]):\n","    for name in files:\n","        listNormalPaths.append(os.path.join(root, name))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WhtzdPMcLAwJ"},"outputs":[],"source":["from distutils.file_util import copy_file\n","\n","# Paths to covid1500 and nomral1500 image folders\n","pathCovid1500 = './COVID1500/'\n","pathNormal1500 = './NORMAL1500/'\n","\n","# Move covid images files to new folders\n","for i in range(1500):\n","      copy_file(listCovidPaths[i], pathCovid1500)\n","      copy_file(listNormalPaths[i], pathNormal1500)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"46BN3QKTrK-M"},"outputs":[],"source":["\n","# Root paths for X-ray Images\n","XRay_covid1500 = glob.glob(pathCovid1500 + '/*.png', recursive = True)\n","XRay_normal1500 = glob.glob(pathNormal1500 + '/*.png', recursive = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Q-BZuKJQZjk"},"outputs":[],"source":["# Time Computing \n","def _time(start, end): \n","    # if in seconds \n","    if (end-start)<60: \n","        wall_time = f'{round((end-start),2)}sec'\n","    # if in minute(s)  \n","    elif (end-start)>=3600: \n","        wall_time = f'{int((end-start)/3600)}h {int(((end-start)%3600)/60)}min {round((end-start)%60,2)}sec'\n","    # if in houre(s)  \n","    else: \n","        wall_time = f'{int((end-start)/60)}min {round((end-start)%60,2)}sec'\n","    return wall_time \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t7zzeJCqQuAL"},"outputs":[],"source":["def get_data(data_path, dim=(128, 128), rand_shuffle=True): \n","    start = time.time() \n","    imgs_data = []         \n","    sample_size = len(data_path)\n","    for idx, im_path in enumerate(data_path): \n","        if idx%(sample_size//10)==0:\n","            print('Processing index {:05d} of {:05d} ==> {:03d}%'\\\n","                  .format(idx, sample_size, round(100*idx/sample_size))) \n","        img = img_to_array(load_img(im_path, target_size = dim)) \n","        imgs_data.append(img) \n","        \n","    # to float \n","    imgs_data = np.array(imgs_data).astype('float32') \n","    # scale to [0,1] (note the . after 255 - float)\n","    imgs_data = imgs_data/255. #for formalizing to [-1,1] ==> (imgs_data - 127.5)/127.5 \n","    \n","    # shuffle the data \n","    if rand_shuffle: \n","        idx = np.arange(imgs_data.shape[0])\n","        np.random.shuffle(idx) \n","        imgs_data = imgs_data[idx,:,:,:] \n","    \n","    print(f\"Hey! the calculations are done in {_time(start, time.time())}\")\n","    return imgs_data  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TXcjCH0yRLpl"},"outputs":[],"source":["## Obtain Data from 1500 training images for covid and normal xray\n","X_covid_1500 = get_data(XRay_covid1500)\n","X_normal_1500 = get_data(XRay_normal1500)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NphDXO-bSBIg"},"outputs":[],"source":["##Parameters for DCGAN taken from Keras DCGAN: https://keras.io/examples/generative/dcgan_overriding_train_step/\n","\n","# Number of training epochs\n","n_epoch = 180\n","batch_size = 128 \n","latent_dim = 100  \n","cols, rows = 299, 299 \n","channels = 3 \n","dim = cols, rows  \n","in_shape = (cols, rows, channels) \n","lr = 0.0002\n","beta1 = 0.5\n","ngpu = 1 \n","nrows, ncols = 3, 4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DFN_4ot6SJQP"},"outputs":[],"source":["#discriminator model taken from Keras DCGAN\n","\n","def define_discriminator(in_shape=(128,128,3)): \n","    model = models.Sequential() \n","    # normal \n","    model.add(layers.Conv2D(128, (5,5), padding='same', input_shape=in_shape)) \n","    model.add(layers.LeakyReLU(alpha=0.2)) \n","    # downsample to 64x64 \n","    model.add(layers.Conv2D(128, (5,5), strides=(2,2), padding='same')) \n","    model.add(layers.LeakyReLU(alpha=0.2)) \n","    # downsample to 32x32 \n","    model.add(layers.Conv2D(128, (5,5), strides=(2,2), padding='same')) \n","    model.add(layers.LeakyReLU(alpha=0.2)) \n","    # downsample to 16x16 \n","    model.add(layers.Conv2D(128, (5,5), strides=(2,2), padding='same')) \n","    model.add(layers.LeakyReLU(alpha=0.2)) \n","    # downsample to 8x8 \n","    model.add(layers.Conv2D(128, (5,5), strides=(2,2), padding='same')) \n","    model.add(layers.LeakyReLU(alpha=0.2)) \n","    # classifier \n","    model.add(layers.Flatten()) \n","    model.add(layers.Dropout(0.4)) \n","    model.add(layers.Dense(1, activation='sigmoid')) \n","    # compile model \n","    opt = optimizers.Adam(lr=0.0002, beta_1=0.5) \n","    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy']) \n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tezvs-FESMQb"},"outputs":[],"source":["#generator model taken from Keras DCGAN\n","\n","\n","def define_generator(latent_dim):\n","    model = models.Sequential()\n","    # foundation for 8x8 feature maps\n","    n_nodes = 128*8*8\n","    model.add(layers.Dense(n_nodes, input_dim=latent_dim))\n","    model.add(layers.LeakyReLU(alpha=0.2))\n","    model.add(layers.Reshape((8, 8, 128)))\n","    # upsample to 16x16\n","    model.add(layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n","    model.add(layers.LeakyReLU(alpha=0.2))\n","    # upsample to 32x32\n","    model.add(layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n","    model.add(layers.LeakyReLU(alpha=0.2))\n","    # upsample to 64x64\n","    model.add(layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n","    model.add(layers.LeakyReLU(alpha=0.2))\n","    # upsample to 128x128\n","    model.add(layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n","    model.add(layers.LeakyReLU(alpha=0.2))\n","    # output layer 128x128x3\n","    model.add(layers.Conv2D(3, (5,5), activation='tanh', padding='same'))\n","    return model \n","\n","# input for Generator (random noise)\n","def generate_latent_points(latent_dim, n_samples):\n","    # generate points in the latent space\n","    x_input = np.random.randn(latent_dim*n_samples)\n","    # reshape into a batch of inputs for the network\n","    x_input = x_input.reshape(n_samples, latent_dim)\n","    return x_input \n","\n","# Generating n_samples number of noise images which will become fake images\n","def generate_fake_samples(g_model, latent_dim, n_samples):\n","    # generate points in latent space\n","    x_input = generate_latent_points(latent_dim, n_samples)\n","    # predict outputs\n","    X = g_model.predict(x_input)\n","    # create 'fake' class labels (0)\n","    y = np.zeros((n_samples, 1))\n","    return X, y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7npZluRFSQHW"},"outputs":[],"source":["#Gan Model taken from Keras DCGAN\n","def define_gan(g_model, d_model): \n","    # make weights in the discriminator not trainable\n","    d_model.trainable = False \n","    # connect them\n","    model = models.Sequential()\n","    # add generator\n","    model.add(g_model)\n","    # add the discriminator\n","    model.add(d_model)\n","    # compile model\n","    opt = optimizers.Adam(lr=0.0002, beta_1=0.5)\n","    model.compile(loss='binary_crossentropy', optimizer=opt)\n","    return model\n","\n","# parse and get real samples (training data)\n","def get_real_samples(dataset, n_samples):\n","    # choose random instances\n","    ix = np.random.randint(0, dataset.shape[0], n_samples)\n","    # retrieve selected images\n","    X = dataset[ix]\n","    # set 'real' class labels (1)\n","    y = np.ones((n_samples, 1))\n","    return X, y\n","\n","# create and save a plot of generated images \n","def show_generated(generated, epoch, covid, nrows=4, ncols=5, num_gan = 1000):\n","    #[-1,1] -> [0,1] \n","    #generated = (generated+1)/2 \n","    #generated = (generated[:ncols*nrows]*127.5)+127.5 \n","    #generated = generated*255 \n","\n","    plt.figure(figsize=(10,10)) \n","    for idx in range(nrows*ncols): \n","        plt.subplot(nrows, ncols, idx+1)\n","        plt.imshow(generated[idx])\n","        plt.axis('off')\n","    if (covid): \n","      plt.savefig('./DCGAN_Covid_gif/image_at_epoch_{:04d}.png'.format(epoch+1), bbox_inches = 'tight', pad_inches = 0) \n","    else:\n","      plt.savefig('./DCGAN_Normal_gif/image_at_epoch_{:04d}.png'.format(epoch+1), bbox_inches = 'tight', pad_inches = 0 )\n","    plt.show() \n","\n","    if (epoch == n_epoch - 1):\n","      for i in range(num_gan):\n","        plt.figure(figsize = (6,6))\n","        plt.imshow(generated[i])\n","        plt.axis('off')\n","        if (covid):\n","          plt.savefig('./DCGAN_Covid/DCGAN_image{:04d}.png'.format(i), bbox_inches = 'tight', pad_inches = 0)\n","        else:\n","          plt.savefig('./DCGAN_Normal/DCGAN_image{:04d}.png'.format(i), bbox_inches = 'tight', pad_inches = 0)\n","\n","# evaluate the discriminator and plot generated images \n","def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, covid = True, n_samples=1000):\n","    # prepare real samples\n","    X_real, y_real = get_real_samples(dataset, n_samples)\n","    # evaluate discriminator on real examples \n","    _, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n","    # prepare fake examples \n","    x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n","    # evaluate discriminator on fake examples \n","    _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n","    # summarize discriminator performance \n","    print('> Accuracy at epoch %d [real: %.0f%%, fake: %.0f%%]'%(epoch+1, acc_real*100, acc_fake*100))\n","    # show plot \n","    show_generated(x_fake, epoch, covid) \n","\n","    \n","def plot_loss(loss):\n","    plt.figure(figsize=(10,5))\n","    plt.title(\"Generator and Discriminator Loss During Training\", fontsize=20) \n","    plt.plot(loss[0], label=\"D_real\") \n","    plt.plot(loss[1], label=\"D_fake\") \n","    plt.plot(loss[2], label=\"G\") \n","    plt.xlabel(\"Iteration\", fontsize=20); plt.ylabel(\"Loss\", fontsize=20) \n","    plt.legend(); plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N36yb9Ul6BUF"},"outputs":[],"source":["!mkdir ./DCGAN_Covid/\n","!mkdir ./DCGAN_Normal/\n","!mkdir ./DCGAN_Covid_gif/\n","!mkdir ./DCGAN_Normal_gif/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iedaD69qSVPZ"},"outputs":[],"source":["\n","def train(g_model, d_model, gan_model, dataset, covid, latent_dim=100, n_epochs=100, n_batch=128):\n","    \n","    start = time.time() \n","    bat_per_epo = int(dataset.shape[0]/n_batch) \n","    half_batch = int(n_batch/2) \n","    loss1, loss2, loss3 = [], [], [] \n","    fake_liste = [] \n","    \n","    # manually enumerate epochs\n","    print('Training Start...')\n","    for i in range(n_epochs):\n","        start1 = time.time()\n","        # enumerate batches over the training set\n","        for j in range(bat_per_epo):\n","            # get randomly selected 'real' samples\n","            X_real, y_real = get_real_samples(dataset, half_batch)\n","            # update discriminator model weights\n","            d_loss1, _ = d_model.train_on_batch(X_real, y_real)\n","            # generate 'fake' examples\n","            X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n","            # update discriminator model weights\n","            d_loss2, _ = d_model.train_on_batch(X_fake, y_fake)\n","            # prepare points in latent space as input for the generator\n","            X_gan = generate_latent_points(latent_dim, n_batch)\n","            # create inverted labels for the fake samples\n","            y_gan = np.ones((n_batch, 1))\n","            # update the generator via the discriminator's error\n","            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n","            # summarize loss on this batch\n","            loss1.append(d_loss1); loss2.append(d_loss2); loss3.append(g_loss) \n","        \n","        print('Epoch: {:03d}/{:03d}, time: {:s}'\\\n","              .format(i+1,n_epochs, _time(start1,time.time())))\n","        # evaluate the model performance \n","        if ((i+1) % 10 == 0): \n","            # Save and show generated images \n","            summarize_performance(i, g_model, d_model, dataset, latent_dim, covid) \n","        \n","    print('Total time for training {} epochs is {} sec'.format(n_epochs, _time(start, time.time())))\n","    \n","    # Show loss curves \n","    loss = (loss1, loss2, loss3) \n","    plot_loss(loss) \n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fq1_yNPjyMcA"},"outputs":[],"source":["\n","## create 1000 dcgan images for covid xray\n","discriminator = define_discriminator() \n","generator = define_generator(latent_dim) \n","gan = define_gan(generator, discriminator)\n","train(generator, discriminator, gan, X_covid_1500, True, latent_dim, n_epoch, n_batch=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wtbXuEqGNvUV"},"outputs":[],"source":["\n","## create 1000 dcgan images for normal xray\n","discriminator = define_discriminator() \n","generator = define_generator(latent_dim) \n","gan = define_gan(generator, discriminator)\n","train(generator, discriminator, gan, X_normal_1500, False, latent_dim, n_epoch, n_batch=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nlh15zu-M2Mo"},"outputs":[],"source":["!zip -r /content/DCGAN_Normal.zip /content/COVID-19_Radiography_Dataset/DCGAN_Normal/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ifmvOl_ANEVR"},"outputs":[],"source":["!zip -r /content/DCGAN_Covid.zip /content/COVID-19_Radiography_Dataset/DCGAN_Covid/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wCI1bQmDv45e"},"outputs":[],"source":["!zip -r /content/DCGAN_Covid_gif.zip /content/COVID-19_Radiography_Dataset/DCGAN_Covid_gif/\n","!zip -r /content/DCGAN_Normal_gif.zip /content/COVID-19_Radiography_Dataset/DCGAN_Normal_gif/"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"DCGAN.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}